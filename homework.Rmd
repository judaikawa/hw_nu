---
title: "Homework Nubank 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This markdown contains the following sections:

1. [Puzzle Overview](#puzzel-overview)
2. [Libraries](#libraries)
3. [Reading the Datasets](#reading-the-datasets)
4. [Data Cleaning](#data-cleaning)
5. [Exploratory Data Analysis](#exploratory-data-analysis)
6. [Modeling](#modeling)
7. [Conclusions](#conclusions)

## Puzzle Overview
As a credit company, it is important to know beforehand who is able to pay their loans and who is not. The goal of this puzzle is to build a statistical/machine learning model to figure out which clients are able to honor their debit.

The goal is to predict the probability of default, which is identified by the default variable in the training set. Thus, we are dealing with a **supervised** problem for a **binary classification** but rather than classes, the output should be the **probability.** The models that it is tested on this notebook are the Logistic Regression and Random Forest.

## Libraries

The following libraries are required to run all the code presented in this notebook.
```{r warning=FALSE, message=FALSE, results='hide'}
library(knitr) # Markdown, LaTeX, HTML
library(dplyr) # data manipulation
library(ggplot2) # plots
library(caret) # classification and regression training
library(kableExtra)
library(DT)
library(corrplot) # for correlation plot
library(tidyr) 
library(randomForest)
library(ROCR)
library(partykit)
```

## Reading the Datasets

```{r}
# Reading the csv format data
train <- read.csv('puzzle_train_dataset.csv')
test <- read.csv('puzzle_test_dataset.csv')

# 
head(train,2) %>% 
  kable(caption='Train Dataset') %>% 
  kable_styling() %>% 
  scroll_box(width = '100%') # adding a scroll bar as it has many columns
```


## Data Cleaning

We have `r nrow(train)` rows in the train dataset with `r ncol(train)` columns. There are some numerical and some categorical variables. Let's take a look at what these variables looks like.

```{r}
# Creating a summary dataframe
summary_df <- data.frame('var' = names(train), 
                         'type' = as.character(lapply(train, class)), 
                         'unique' = as.character(lapply(train, function(x) length(unique(x)))), 
                         'count_na_values' = as.character(lapply(train, function(x) sum(is.na(x)))),
                         'first_obs' = as.character(lapply(train, function(x) as.character(first(x)))))

summary_df %>% 
  kable(caption='Variables Summary') %>% 
  kable_styling() %>% 
  scroll_box(width = '100%', height = '50px')

```

Looking at the number of unique values for each feature, we can already tell that each row represents an unique client (*ids*). Note also that there are some encrypted variables. We also have some missing values, marked on the *count_na_values* column. We also see that the variable *borrowed_in_months* is numerical but only has 3 values, so it may be better to use it as a categorical variable.
```{r}
train$borrowed_in_months <- as.factor(train$borrowed_in_months)
test$borrowed_in_months <- as.factor(test$borrowed_in_months)
```


Our dependent variable (target) is *default*, which is a boolean value. However, we see that there are 3 levels. Inspecting, we find that there are empty imputs creating a "" level, and that this occurs to every factor variable except the *ids* column. These will be treated as missing values, as it makes no sence in none of the features, and is dangerous to keep it this way as it could lead to the model using all n dummy variables instead of (n-1).
```{r}
# Structure of the categorical variables
str(select_if(train, is.factor)) # Undesirable empty level ""
summary(train$default) # count of each level in the target variable

# Number of empty cells in the train dataset
sum(train=="", na.rm = T)

# Creating a functin to clean the empty values, as it needs to be applied to both train and test dataset 
cleaning_empty_levels <- function(data) {
  data[data==""] <- NA

  # Removing "" levels from factors
  for (column in names(select_if(data, is.factor))) {
    data[,column] <- droplevels(data[,column])
  }
  
  return(data)
}

train <- cleaning_empty_levels(train)
test <- cleaning_empty_levels(test)
```

We will also remove the rows that the *default* variable is NA, as the model can't learn anything from them.
```{r}
train <- train[-which(is.na(train$default)),]
```

## Exploratory Data Analysis

### Correlation

From the correlation plot of the numerical variables, we see that the the variables *n_accounts* and *n_issues* are highly correlated, with a value of `r round(cor(train$n_accounts, train$n_issues, use="complete.obs"),4)`. Other than that, we don't see any strong correlation between other variables.
```{r}
corrplot(cor(select_if(train, is.numeric), use = "complete.obs"))
```

### Missing Values
```{r}
round(sapply(train, function(x) sum(is.na(x)))/nrow(train), 3)
```

The column *ok_since* will be dropped as it contains more than 50% of missing values; the column *channel* will be dropped as it only has 1 value to the entire dataset and the column *sign* will also be discarded as it has 38% of missing value and someone's zodiac sign shouldn't be relevant to this study. Also, the column *n_issues* will be dropped as we seen that it is highly correlated to *n_accounts* and has 26% of missing value while the first has 0%.
```{r}
train <- train[,-which(names(train) %in% c("ok_since","channel", "sign", "n_issues"))]
```

### Distributions

Looking at the density of the numerical variables, we see that some are highly skewed. We may want to look closer as we may find some potential outliers.
```{r}
select_if(train, is.numeric) %>% 
  gather() %>%  
  ggplot(aes(value)) + 
  facet_wrap(~ key, scales='free') + 
  geom_density()
```

```{r}
select_if(train, is.numeric) %>% 
  gather() %>%  
  ggplot(aes(y=value)) + 
  facet_wrap(~ key, scales='free') + 
  geom_boxplot()
```

*n_bankruptcies* and *n_defaulted_loans* are discrete and have very low representation of higher values. Let's change it to binary variables instead.
```{r}
train$n_bankruptcies <- as.factor(ifelse(train$n_bankruptcies==0,0,1))
train$n_defaulted_loans <- as.factor(ifelse(train$n_defaulted_loans==0,0,1))
test$n_bankruptcies <- as.factor(ifelse(test$n_bankruptcies==0,0,1))
test$n_defaulted_loans <- as.factor(ifelse(test$n_bankruptcies==0,0,1))
```


## Modeling

As the No Free Lunch theorem states, there is no such thing as "the best model". So we will build a Logistic Regression and a Random Forest and evaluate them to find the optimal one for our problem.

We also need to be aware of how the dataset is **imbalanced** with respect to the target variable, as only `r round(sum(train$default=='True', na.rm = T)/nrow(train), 2)*100`% of the target is labeled *True.* This needs to be taken into account as looking only at metrics like Accuracy may not be a good indicator, alternatively, we should look at a good tradeoff between Sensitivity and Specificity.
```{r}
summary(train$default)
```

### Metrics

As our goal is to predict the probability of default, that is, the probability of the output being of the class *True*, the following metrics will be used to evaluate our models: AUC (Area Under the ROC Curve), R squared and the Log Loss Score.
```{r}
# R squared
r_squared <- function(model) {
  return(round( 1 - ( summary(model)$deviance / summary(model)$null.deviance ), 3 ))
}

# Log Loss
logLoss <- function(pred, actual){
  ll <- -1*mean(log(pred[model.matrix(~ actual + 0) - pred > 0]))
  return(round(ll,3))
}

# ROC and AUC
roc_auc <- function(pred, actual) {
  metrics <- ROCR::prediction(pred, actual)
  
  perf <- performance(metrics, measure="tpr",
                    x.measure="fpr")
  auc <- performance(metrics, measure="auc")
  
  return(plot(perf, main = paste0("AUC: ", auc@y.values)) + abline(a=0,b=1))
}

# AUC
auc <- function(pred, actual) {
  metrics <- ROCR::prediction(pred, actual)
  auc <- performance(metrics, measure="auc")
  return(round(unlist(auc@y.values),3))
}

```


We could additionally look at how it performns as a binary classifier, with metrics like Recall, Precision, F1-Score and Confusion Matrix. We would however need to define a threshold on this case.

### Handling with Imbalanced Dataset
As stated, we should be cautious with the fact that the 2 classes on the target variable aren't equally represented on the dataset. A simple way to overcome this is to use resampling methods like Undersampling and Oversampling. Because we have not many data, we'll go with the latter. 
Another good way to handle this is by defining a cost function. We first need to wonder if classifying a customer as default when in reality he is not is better than the other way around. But because this needs more understanding of the business, I'll leave it as a side note.

Let's change the target to 0 and 1 to make it easier for further evaluations. 
```{r}
train$default <- ifelse(train$default=='True',1,0)
train$default <- as.factor(train$default)
```


### Cross Validation
The first step is to split the train dataset into the train the model will receive as input and a holdout validation dataset to evaluate the quality of the model on unseen data. We'll use a 70-30 split.
```{r}
set.seed(123) 

train_idx <- sample(nrow(train), nrow(train)*0.7)
test_idx <- setdiff(seq(1, nrow(train)), train_idx)

validation <- train[test_idx,]
train <- train[train_idx,]
```


Now, we need to do an oversample of out training dataset so that the model won't be biased by the majority  class. A side note to oversampling however is that it can lead to overfitting, as it adds replicated observations in the original data set.
```{r}
set.seed(456)
dados0 <- subset(train, train$default==0)
dados1 <- subset(train, train$default==1)
amostra0 <- sample(nrow(dados0), nrow(dados0))
amostra1 <- sample(nrow(dados1), nrow(dados0), replace = TRUE)

amostra0 <- dados0[amostra0,]
amostra1 <- dados1[amostra1,]

sample_data <- rbind(amostra0, amostra1)
rm(amostra0, amostra1)
```

### Logistic Regression

As the desired output is the probability, we'll start by modeling a Logistic Regression with a Stepwise Feature Selection.

The features *job_name*, *reason*, *zip* and *state* have too many levels, and a more deep analysis would be needed to see if it could be broken down into smaller levels (perhaps in a clustering analysis) and if it would be worth it. As for in this project, it will be leaved out.

A transformation on the variables *credit_limit* and *income* were made to try to make it fit better, in an attempt to make their skewness lower.
```{r}
full <- default ~ score_1 + score_2 + score_3 + score_4 + score_5 + score_6 + risk_rate + amount_borrowed + borrowed_in_months + sqrt(credit_limit) + log(income) + gender + facebook_profile + real_state + n_bankruptcies + n_defaulted_loans + n_accounts

glm <- glm(full, sample_data, family=binomial())

glm <- step(glm, trace=FALSE)

formula <- glm$formula

pred_glm <- predict(glm, validation, type='response')

# Metrics
r_glm <- r_squared(glm)
auc_glm <- auc(pred_glm, validation$default)
logloss_glm <- logLoss(pred_glm[-which(is.na(pred_glm))],
                       ifelse(validation$default==0,0,1)[-which(is.na(pred_glm))])
```

### Logistic Regression with Binary Predictors

Another Logistic Regression was tested but only with binary variables. A decision tree was built to every variable in the dataset and its splits were made to categorize the numerical variables and reduce the levels of the categories with high dimensionality. Unfortunatly, it took way too many time to process the tree for categories with more than 30 levels, so that was left out. This served more of a test, and it didn't give any improvement to the last model.
```{r}
binary_transf <- function(data) { 
  binary_data <- data.frame('default'=data$default)

  for (var in c('score_1','score_3','score_4','score_5','score_6','risk_rate','amount_borrowed','borrowed_in_months','credit_limit','income','gender','facebook_profile','real_state','n_bankruptcies','n_defaulted_loans','n_accounts')) {
    
    # Decision Trees to find splitting rules
    tree <- partykit::ctree(default~eval(parse(text=var)), sample_data)
    rules <- list(partykit:::.list.rules.party(tree))
    if (length(unlist(rules))>1) {
      for (i in 1:length(unlist(rules))) {
        newVar <- paste0(var,'_',i)
        binary <- data %>% 
          mutate(newVar=ifelse(eval(parse(text = unlist(rules)[1])) ,1,0)) %>% select(newVar)
        names(binary) <- newVar
        binary_data <- cbind(binary_data, binary)
      }
    }
  }
  
  return(binary_data)
}

binary_train <- binary_transf(sample_data)
binary_validation <- binary_transf(validation)


glm_bin <- glm(default~., binary_train, family=binomial())

pred_glm_bin <- predict(glm_bin, binary_validation, type='response')

# Metrics
r_glm_bin <- r_squared(glm_bin)
auc_glm_bin <- auc(pred_glm_bin, binary_validation$default)
logloss_glm_bin <- logLoss(pred_glm_bin[-which(is.na(pred_glm_bin))],
                       ifelse(binary_validation$default==0,0,1)[-which(is.na(pred_glm_bin))])
```

### Random Forest

Finally, a Random Forest was built to see if it perfoms better than the Logistic Regression.
```{r}
formula <- default ~ score_1 + score_2 + score_3 + score_4 + score_5 + score_6 + risk_rate + amount_borrowed + borrowed_in_months + credit_limit + income + gender + facebook_profile + real_state + n_bankruptcies + n_defaulted_loans + n_accounts

rf <- randomForest(formula, na.omit(sample_data))

pred_rf <- predict(rf, validation, type='prob')
pred_rf <- pred_rf[,2]

# Metrics
r_rf <- 1 - sum((ifelse(validation$default==0,0,1)-pred_rf)^2, na.rm = T)/sum((ifelse(validation$default==0,0,1)-mean(ifelse(validation$default==0,0,1)))^2, na.rm = T)
auc_rf <- auc(pred_rf, validation$default)
logloss_rf <- logLoss(pred_rf[-which(is.na(pred_rf))],
                       ifelse(validation$default==0,0,1)[-which(is.na(pred_rf))])

varImpPlot(rf)
```

From the plot of the importance of the variables above, we see that the variables that impacts the most in the target variable are: *score_2*, *income* and *amount_borrowed*.


## Conclusions

Below is the table with all the metrics of the 3 models built. 
```{r}
df <- data.frame('model' = c('Logistic Reg.', 'Logistic Reg. Binary Predictors', 'Random Forest'), 
           'R squared' = c(r_glm, r_glm_bin, r_rf), 
           'AUC' = c(auc_glm, auc_glm_bin, auc_rf), 
           'Log Loss' = c(logloss_glm, logloss_glm_bin, logloss_rf))

df %>% 
  kable(caption='Metrics') %>% 
  kable_styling()
```

The metrics R squared and AUC are better when higher (best value equals to 1) and the Log Loss Score is better when lower (best value equals to 0). So we choose the Logistic Regression. 
```{r}
# ROC plot of the chosen model
roc_auc(pred_glm, validation$default)
```


## Creating the csv file
```{r}
final_predictions <- data.frame('ids' = test$ids)
final_predictions$predictions <- predict(glm,test)

write.csv(final_predictions, 'predictions.csv')
```

A final note is that the model built uses predictors that is not available for all clients in the test dataset, and for these cases it is labeled NA. In this case, a second model would be needed or collecting more data/different features.


